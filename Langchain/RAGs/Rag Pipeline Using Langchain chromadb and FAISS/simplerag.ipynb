{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Ingest/Load data from a txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'sample.txt'}, page_content='The drumbeat of pressure on Joe Biden to drop out of the US presidential race intensified Wednesday with a bombshell report in the New York Times that he had conceded the possibility to a key ally, as well as movement within his own party to demand his withdrawal.\\nThe White House and Biden\\'s campaign quickly denied the Times report suggesting the president had vocalized to a supporter that he could ill-afford another misstep that would irrevocably damage his campaign. Biden himself insisted to campaign staff he intended to remain in the race.\\n\\n\"I\\'m in this race to the end and we\\'re going to win because when Democrats unite, we will always win,\" Biden said in a call alongside Vice President Kamala Harris.\\nYet time is running out for the beleaguered president to convince anxious Democratic officials, donors and voters that he remains viable in his effort to keep former President Donald Trump from returning to office. In another blow, dozens of Democratic lawmakers are considering signing a letter demanding Biden withdraw from the race, a senior party official said.\\n\\nThat anxiety has only been fueled by a flood of recent reporting suggesting other Democrats are eyeing possible replacement candidates - and by the Times reporting.\\n\\nBiden told his ally the race would be in a \"different place\" if upcoming events went poorly, the Times reported. White House Press Secretary Karine Jean-Pierre subsequently said Biden had flatly denied making such a comment.\\n\\nBiden plans to sit for an interview with ABC News on Friday, and hold a rally in Madison, Wisconsin. On Sunday, he\\'ll travel to Philadelphia for another campaign event. He also plans interviews with Black radio stations in Philadelphia and Milwaukee to coincide with his travel.\\n\\nBiden has been calling senior Democratic lawmakers - including Senate Majority Leader Chuck Schumer and House Minority Leader Hakeem Jeffries - in a bid to shore up support on Capitol Hill, even as members of his party are publicly expressing dismay about his campaign.\\n\\nSo far, only two sitting House Democrats - Lloyd Doggett of Texas and Raul Grijalva of Arizona - have publicly called for Biden to step aside. But the President may not be able to survive a coordinated revolt among Democratic lawmakers worried that his poor performance could cost them seats or a shot at control of the House and Senate in the upcoming election.\\n\\nJean-Pierre said Biden had told her the calls with congressional Democrats were \"strong.\"\\n\\n\"He\\'s moving forward as being president. He\\'s moving forward with his campaign,\" she added.\\n\\nA Senate Democrat, however, said Wednesday evening that several colleagues had privately indicated they didn\\'t see a way for the president to survive politically. The senator, granted anonymity to speak frankly about discussions among colleagues, said Biden hadn\\'t assuaged concerns about his debate collapse against Trump.\\n\\nBiden served for 36 years in the Senate, and Democrats there have been largely silent about his candidacy during a week of turmoil.\\n\\nLater on Wednesday, Biden held a hastily arranged meeting with Democratic governors, many of whom are at the center of speculation about possibly replacing him on the ticket. Several emerged to say that they were firmly behind Biden. \"The president was very clear that he is in this to win this,\" Governor Wes Moore of Maryland told reporters.')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader('sample.txt')\n",
    "text = loader.load()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Load the environment variables</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY']=os.getenv('OPENAI_API_KEY')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Load the data from a html page</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://www.analyticsvidhya.com/blog/2013/10/read-books-web-analytics/'}, page_content=\"By reading something every day before sleeping, I not only continue my learning, but also end my day on a fulfilling note. This is third and final post in series of must read books (click here for\\xa0Business Analysts\\xa0and\\xa0visualization).The demand for web analytics professionals is set to increase. Here are some of the reasons why I expect so:All these reasons are enough to ensure that application of Web Analytics grows by leaps and bounds. With increased application, the world would need more professionals in this industry.So, if you plan a career in Analytics and are not up to pace with Web Analytics, start learning it today. It is a dynamic field where things change frequently. Keeping that in mind, this list contains both books and blogs, as I think they need to be used simultaneously for effective learning.There is a plethora of reading material on this subject. However, I have shortlisted only the best resources which would definitely add value to your web analytics learning. All these books and blogs are tremendous source of knowledge and have helped me increase my understanding of web Analytics:\\xa0[stextbox id=”section”] Books and blog by Avinash Kaushik [/stextbox]Meet Avinash Kaushik, Digital Marketing Evangelist at Google, author of some of the best books on web analytics, blogger – Occam’s Razor (one of the most popular blog on web Analytics) and co-founder of Market Motive Inc.No list of resources on Web Analytics can be complete without mentioning work of Avinash Kaushik. Whether you are new to web analytics or a pro in this field, you will still find tons of practical advice on his blog or in his books.Here is a brief description of the books he has written:Web Analytics: An Hour a DayThrough this step-by-step guide on how to implement a successful Web analytics strategy, Kaushik leads you on a path to gaining actionable insights from your analytics efforts. Discover how to move beyond clickstream analysis, why qualitative data should be your focus, and more insights and techniques that will help you develop a customer-centric mind-set without sacrificing your company’s bottom line.Web Analytics 2.0This book starts where the first one ended (though you can read them independently). It provides specific recommendations for creating an actionable strategy, applying analytical techniques correctly, solving challenges such as measuring social media and multi-channel campaigns, achieving optimal success by leveraging experimentation, and employing tactics for truly listening to your customers. The book will help you become a super analysis ninja!\\xa0[stextbox id=”section”] Advanced Web Metrics with Google Analytics by Brian Clifton [/stextbox]This book can be easily called out as Bible of Google Analytics. Brian formerly led the Google Web Analytics team for Europe, the Middle East, and Africa, and he initiated and helped launch the online learning center for the Google Analytics Individual Qualification (GAIQ).This book will teach you each and every aspect of Google Analytics, starting from basic reporting to creating segmentation, tracking social and mobile visitors, use of new multichannel funnel reporting features and much more. If there is only one book you want to read on Google Analytics, this is your book.\\xa0[stextbox id=”section”]How to Measure Social Media: A Step-By-Step Guide to Developing and Assessing Social Media ROI by Nichole Kelly\\xa0[/stextbox]If you face any of these questions and do not have a concrete answer, this book will provide you the answers:This book will give you simple step-by-step techniques for creating measurable strategies and getting the data to prove they deliver. You’ll also get helpful hands-on exercise worksheets.[stextbox id=”section”]List of blogs to follow:[/stextbox]1. Occam’s Razor by Avinash Kaushik2. Analytics Talk by James Cutroni3. Official Google Analytics blog4. Adobe digital Marketing blog (formerly known as Omniture Industry Insights)5. Since I have not included any text on search, Search Engine Watch becomes a must follow to be up to speed with development on Search[stextbox id=”section”]Special mention (book):[/stextbox]Search Analytics for Your Site: Conversations with Your Customers by Louis Rosenfeld\\xa0was a very useful book when I read it. However, given the recent rise in “Not Available” keywords on search, I have strayed away from recommending it as must read.On my reading list:Web Analytics Action Hero: Using Analysis to Gain Insight and Optimize Your Business by Brent DykesWhat do you think about the list? Are there any other recommendations you would want to share on the subject? Please add them in the comments below.You might also like:\\nKunal is a post graduate from IIT Bombay in Aerospace Engineering. He has spent more than 10 years in field of Data Science. His work experience ranges from mature markets like UK to a developing market like India. During this period he has lead teams of various sizes and has worked on various tools like SAS, SPSS, Qlikview, R, Python and Matlab. \\nMust read books for Analysts (or people interested... \\nLearning path & resources to start your data ... \\nMust have books for data scientists (or aspiring o... \\nAnalytics training recommendations from last 2 months \\n9 Books to Start Your Business Analytics Journey \\nAll you need to know to start a career in analytics \\nHow to apply web analytics for e-Commerce websites? \\nStartups bringing analytics and data science close... \\nInfographic: Must Read Books in Analytics / Data S... \\n13 Tips to make you awesome in Data Science / Anal... Lorem ipsum dolor sit amet, consectetur adipiscing elit,ClearSubmit reply \\n\\nΔ\\nHey Kunal\\nThanks for rudiment suggetions. But for a bignner I think before putting hand in web analytics one must know basics of data analytics; Please can you suggest some books for it. ClearSubmit reply \\n\\nΔ\\nShaifali,\\nLook at the follwing article\\nKunal ClearSubmit reply \\n\\nΔ\\nHi Kunal,\\nI have made entry into analytics field recently after 3 yrs of experience in Market Research. Initially I started with learning statistical techniques through SPSS and SAS, later moved on to SQL.Moving on, Since I am interested in marketing analytics , Can you please suggest me'\\n1)Main statistical techniques or modelling techniques that I should learn (like segmentation,clustering,time series modelling) that would suit marketing analytics' common needs\\n2)Tools that I could learn in Marketing Analytics (Web/Social media Analytics tools such as Google analytics, Omniture, Core Metrics). Which would be the best to learn?\\n3)Would it be good to learn big data technologies like R or Hadoop which find greater use in web crawling,sentiment analytics and marketing analytics than learning marketing analytics specific tools like Omniture or Sysomos?\\nIt would be really helpful if you could help me in identifying them.\\n-Janani ClearSubmit reply \\n\\nΔ\\nJanani,\\nWelcome to this wonderful world! Here are your answers:\\n1. Segmentation and clustering are the most common used techniques in marketing. Other popular techniques is predictive modeling (mostly regression) for Cross-sell and up-sell. If your employer sells products online / has a big online presence, web analytics is another are to lookout for.\\n2. Start with GA and then try one of the two - Omniture / Coremetric\\n3. I wouldn't trade-off one over other. You should learn depending on the need and the role you are currently performing / plan to take up. If it involves web, go for basic web analytics first and big data later on. If not, do it other way.\\nThanks,\\nKunal ClearSubmit reply \\n\\nΔ\\nHi Kunal,\\nFirst of all I thank you for the great guidance that you and your team is providing to poeple in analytics and people who want to enter in analytics. I need some guidance from you.\\nCurrently I am working in Web analytics & digital marketing domain. I have 6 months of experience. I have also undergone training in business analytics with sas (and I love it ). I want to make a switch to business analytics, but since I dont have any experience in business analytics, so i am facing problems in switching. I asked some people regarding this issue, they suggested me to stick with web analytics domain as it is bound to achieve great heights especially in india. But if I compare salaries of people in web analytics domain with people in business analytics domain (from payscale.com) there is a significant difference, with people in business analytics earning great packages. Further I have read on some blog that salaries in web analytics saturate around 10-12 LPA. Whats your opinion on this? is this correct ? and What would you suggest, should continue my efforts of switching to business analytics or stick with web analytics? ClearSubmit reply \\n\\nΔ\\nAbhi,\\nYou are absolutely right about the difference in the salary and its saturation. I think you should continue to look out for BA role.\\nWhere are you based out of? Can you mail me your CV? I might be aware of an opening, which might be relevant to you.\\nThanks,\\nKunal ClearSubmit reply \\n\\nΔ\\nGreat blog about Business Analytics, easily understandable.\\nCurrently am in Pre sales role with a year experience. Am reading much about this topic and role, its very interesting. And today I found this interesting blog as well to dig in this field. Literally am fresh to this field, I ll start reading with above mentioned books and contents in the blog. I will make into this field for sure. Kunal sir I will definitely seek assistance after better understanding of this topic.\\nThank You. ClearSubmit reply \\n\\nΔWrite, captivate, and earn accolades and rewards for your workRahul ShahSion ChakrabartiCHIRAG GOYALBarney DarlingtonSuvojit HoreArnab MondalPrateek Majumder\\nTerms & conditions\\n\\n\\n\\nRefund Policy\\n\\n\\n\\nPrivacy Policy\\n\\n\\n\\nCookies Policy\\n© Analytics Vidhya 2024.All rights reserved.\\n A verification link has been sent to your email id  If you have not recieved the link please goto\\nSign Up  page again\\nThis email id is not registered with us. Please enter your registered email id.\")]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from bs4 import SoupStrainer\n",
    "loader = WebBaseLoader(web_path='https://www.analyticsvidhya.com/blog/2013/10/read-books-web-analytics/',\n",
    "              bs_kwargs={'parse_only':SoupStrainer('p')}\n",
    "              )\n",
    "web_documents = loader.load()\n",
    "web_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Load data from a PDF File</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'NLP.pdf', 'page': 0}, page_content='Steve Nouri                          https://www.linkedin.com/in/stevenouri/  Top 100 NLP Questions \\n     Steve Nouri  \\n \\n \\nQ1. Which of the following techniques can be used for keyword normalization in \\nNLP, the process of converting a keyword into its base form? \\na. Lemmatization \\nb. Soundex \\nc. Cosine Similarity \\nd. N-grams \\n \\nAnswer : a) Lemmatization helps to get to the base form of a word, e.g. are playing -> play, ea ting \\n-> eat, etc.Other options are meant for different purposes. \\n \\nQ2. Which of the following techniques can be used to compute the distance \\nbetween two word vectors in NLP? \\na. Lemmatization \\nb. Euclidean distance \\nc. Cosine Similarity \\nd. N-grams \\n \\nAnswer : b) and c) \\nDistance between two word vectors can be computed using Cosine similarity and Euclidean \\nDistance.  Cosine Similarity establishes a cosine angle between the vector of two words . A cosi ne \\nangle close to each other between two word vectors indicates the words are simil ar and vice a \\nversa. \\nE.g. cosine angle between two words “Football” and “Cricket” will be closer to 1 as co mpared to \\nangle between the words “Football” and “New Delhi”  \\n \\nQ3. What are the possible features of a text corpus in NLP? \\na. Count of the word in a document \\nb. Vector notation of the word \\nc. Part of Speech Tag \\nd. Basic Dependency Grammar \\ne. All of the above \\n \\nAnswer : e)All of the above can be used as features of the text corpus. \\n \\n \\nQ4. You created a document term matrix on the input data of 20K documents for a \\nMachine learning model. Which of the following can be used to reduce the \\ndimensions of data? \\n'),\n",
       " Document(metadata={'source': 'NLP.pdf', 'page': 1}, page_content='Steve Nouri                          https://www.linkedin.com/in/stevenouri/  1. Keyword Normalization \\n2. Latent Semantic Indexing \\n3. Latent Dirichlet Allocati on \\n \\na. only 1 \\nb. 2, 3 \\nc. 1, 3 \\nd. 1, 2, 3 \\n  \\nAnswer : d) \\n \\nQ5. Which of the text parsing techniques can be used for noun phrase detection, \\nverb phrase detection, subject detection, and object detection in NLP. \\na. Part of speech tagging \\nb. Skip Gram and N-Gram extraction \\nc. Continuous Bag of Words \\nd. Dependency Parsing and Constituency Parsing \\n \\nAnswer : d) \\n \\nQ6. Dissimilarity between words expressed using cosine similarity will have values \\nsignificantly higher than 0.5 \\na. True \\nb. False \\n \\nAnswer : a) \\n \\nQ7. Which one of the following are keyword Normalization techniques in NLP \\na.  Stemming \\nb.  Part of Speech \\nc. Named entity recognition \\nd. Lemmatization \\n \\nAnswer : a) and d) \\nPart of Speech (POS) and Named Entity Recognition(NER) are not keyword Norm alization \\ntechniques. Named Entity help you extract Organization, Time, Date, City, etc..t ype of entities \\nfrom the given sentence, whereas Part of Speech helps you extract Noun, Verb, Pronoun, \\nadjective, etc..from the given sentence tokens. \\n \\nQ8. Which of the below are NLP use cases? \\na. Detecting objects from an image \\nb. Facial Recognition \\nc. Speech Biometric \\nd. Text Summarization \\n'),\n",
       " Document(metadata={'source': 'NLP.pdf', 'page': 2}, page_content='Steve Nouri                          https://www.linkedin.com/in/stevenouri/   \\nAnswer :  (d) \\na) And b) are Computer Vision use cases, and c) is Speech use case. \\nOnly d) Text Summarization is an NLP use case. \\n \\nQ9. In a corpus of N documents, one randomly chosen document contains a total \\nof T terms and the term “hello” appears K times.  \\nWhat is the correct value for the product of TF (term frequency) and IDF (inverse-docu ment-\\nfrequency), if the term “hello” appears in approximately o ne-third of the total documents? \\na. KT * Log(3) \\nb. T * Log(3) / K \\nc. K * Log(3) / T \\nd. Log(3) / KT \\n \\nAnswer : (c) \\nformula for TF is K/T \\nformula for IDF is log(total docs / no of docs containing “data”)  \\n= log(1 / (⅓))  \\n= log (3) \\nHence correct choice is Klog(3 )/T \\n \\nQ10. In NLP, The algorithm decreases the weight for commonly used words and \\nincreases the weight for words that are not used very much in a collection of \\ndocuments \\na. Term Frequency (TF) \\nb. Inverse Document Frequency (IDF) \\nc. Word2Vec \\nd. Latent Dirichlet Allocation (LDA) \\n \\nAnswer : b) \\n \\n \\n \\n \\n \\nQ11. In NLP, The process of removing words like “and”, “is”, “a”, “an”, “the” from \\na sentence is called as \\na. Stemming \\nb. Lemmatization \\nc. Stop word \\nd. All of the above \\n \\nAnswer : c) In Lemmatization, all the stop words such as a, an, the, etc.. are removed . One can \\nalso define custom stop words for removal. \\n'),\n",
       " Document(metadata={'source': 'NLP.pdf', 'page': 3}, page_content='Steve Nouri                          https://www.linkedin.com/in/stevenouri/   \\nQ12. In NLP, The process of converting a sentence or paragraph into tokens is \\nreferred to as Stemming \\na. True \\nb. False \\n \\nAnswer : b) The statement describes the process of tokenization and not stemming, hence it  is \\nFalse. \\n \\nQ13. In NLP, Tokens are converted into numbers before giving to any Neural \\nNetwork \\na. True \\nb. False \\n \\nAnswer : a) In NLP, all words are converted into a number before feeding to a Neural Network. \\n \\nQ14 Identify the odd one out \\na. nltk \\nb. scikit learn \\nc. SpaCy \\nd. BERT \\n \\nAnswer : d) All the ones mentioned are NLP libraries except BERT, which is a word embedding \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nQ15 TF-IDF helps you to establish? \\na. most frequently occurring word in the document \\nb. most important word in the document \\n \\nAnswer : b) TF-IDF helps to establish how important a particular word is in the contex t of the \\ndocument corpus. TF-IDF takes into account the number of times the word appears in the \\ndocument and offset by the number of documents that appear in the corpus. \\n● TF is the frequency of term divided by a total number of terms in the docu ment. \\n● IDF is obtained by dividing the total number of documents by the number of documen ts \\ncontaining the term and then taking the logarithm of that quotient. \\n'),\n",
       " Document(metadata={'source': 'NLP.pdf', 'page': 4}, page_content='Steve Nouri                          https://www.linkedin.com/in/stevenouri/  ● Tf.idf is then the multiplication of two values TF and IDF. \\n \\nQ16 In NLP, The process of identifying people, an organization from a given \\nsentence, paragraph is called \\na. Stemming \\nb. Lemmatization \\nc. Stop word removal \\nd. Named entity recognition \\n \\nAnswer : d) \\n \\nQ17 Which one of the following is not a pre-processing technique in NLP \\na. Stemming and Lemmatization \\nb. converting to lowercase \\nc. removing punctuations \\nd. removal of stop words \\ne. Sentiment analysis \\n \\nAnswer : e) Sentiment Analysis is not a pre-processing technique. It is done after pr e-processing \\nand is an NLP use case. All other listed ones are used as part of statement pre-processing.  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nQ18 In text mining, converting text into tokens and then converting them into an \\ninteger or floating-point vectors can be done using \\na. CountVectorizer \\nb.  TF-IDF \\nc. Bag of Words \\nd. NERs \\n \\nAnswer : a) CountVectorizer helps do the above, while others are not applicable. \\ntext =[“Rahul is an avid writer, he enjoys studying understanding and presen ting. He loves to \\nplay”]  \\nvectorizer = CountVectorizer() \\nvectorizer.fit(text) \\nvector = vectorizer.transform(text) \\n'),\n",
       " Document(metadata={'source': 'NLP.pdf', 'page': 5}, page_content='Steve Nouri                          https://www.linkedin.com/in/stevenouri/  print(vector.toarray()) \\noutput  \\n[[1 1 1 1 2 1 1 1 1 1 1 1 1 1]] \\nThe second section of the interview questions covers advanced NLP techniques such as \\nWord2Vec, GloVe  word embeddings, and advanced models such as GPT, ELMo, BERT, XLNET \\nbased questions, and explanations. \\n \\nQ19. In NLP, Words represented as vectors are called as Neural Word Embedding s \\na. True \\nb. False \\nAnswer : a) Word2Vec, GloVe based models build word embedding vectors that are \\nmultidimensional. \\n \\nQ20. In NLP, Context modeling is supported with which one of the following word \\nembeddings \\n1. a. Word2Vec \\n2. b) GloVe \\n3. c) BERT \\n4. d) All of the above \\nAnswer : c) Only BERT (Bidirectional Encoder Representations from Transformer) support s \\ncontext modelling where the previous and next sentence context is taken into consider ation. In \\nWord2Vec, GloVe only word embeddings are considered and previous and next sentence context \\nis not considered. \\n \\n \\n \\n \\n \\n \\nQ21. In NLP, Bidirectional context is supported by which of the following \\nembedding \\na. Word2Vec \\nb. BERT \\nc. GloVe \\nd. All the above \\nAnswer : b) Only BERT provides a bidirectional context. The BERT model uses the previous and \\nthe next sentence to arrive at the context.Word2Vec and GloVe are word embeddings, they do \\nnot provide any context.  \\n \\nQ22. Which one of the following Word embeddings can be custom trained for a \\nspecific subject in NLP \\na. Word2Vec \\nb. BERT \\n'),\n",
       " Document(metadata={'source': 'NLP.pdf', 'page': 6}, page_content='Steve Nouri                          https://www.linkedin.com/in/stevenouri/  c. GloVe \\nd. All the above \\n \\nAnswer : b) BERT allows Transform Learning on the existing pre-trained models and hence can  \\nbe custom trained for the given specific subject, unlike Word2Vec and GloVe where existing word \\nembeddings can be used, no transfer learning on text is possible. \\n \\nQ23. Word embeddings capture multiple dimensions of data and are represented \\nas vectors \\na. True \\nb. False \\n \\nAnswer : a) \\n \\nQ24. In NLP, Word embedding vectors help establish distance between two tokens \\na. True \\nb. False \\n \\nAnswer : a) One can use Cosine similarity to establish distance between two vectors represented \\nthrough Word Embeddings \\n \\nQ25. Language Biases are introduced due to historical data used during training of \\nword embeddings, which one amongst the below is not an example of bias \\na. New Delhi is to India, Beijing is to China \\nb. Man is to Computer, Woman is to Homemaker \\n \\nAnswer : a) \\nStatement b) is a bias as it buckets Woman into Homemaker, whereas statement  a) is not a \\nbiased statement. \\n \\nQ26. Which of the following will be a better choice to address NLP use cases such \\nas semantic similarity, reading comprehension, and common sense reasoning \\na. ELMo \\nb. Open AI’s GPT  \\nc. ULMFit \\n \\nAnswer :  b) Open AI’s GPT is able to learn complex pattern in data by using the Tr ansformer \\nmodels Attention mechanism and hence is more suited for complex use cases such as semantic \\nsimilarity, reading comprehensions, and common sense reasoning. \\n \\nQ27. Transformer architecture was first introduced with? \\na. GloVe \\nb. BERT \\n'),\n",
       " Document(metadata={'source': 'NLP.pdf', 'page': 7}, page_content='Steve Nouri                          https://www.linkedin.com/in/stevenouri/  c. Open AI’s GPT  \\nd. ULMFit \\n \\nAnswer : c) ULMFit has an LSTM based Language modeling architecture. This got replaced into \\nTransformer architecture with Open AI’s GPT  \\n \\nQ28. Which of the following architecture can be trained faster and needs less \\namount of training data \\na. LSTM based Language Modelling \\nb. Transformer architecture \\n \\nAnswer :  b) Transformer architectures were supported from GPT onwards and were faster to \\ntrain and needed less amount of data for training too. \\n \\nQ29. Same word can have multiple word embeddings possible with ____________? \\na. GloVe \\nb. Word2Vec \\nc. ELMo \\nd. nltk \\n \\nAnswer : c) EMLo word embeddings supports same word with multiple embeddings, this help s \\nin using the same word in a different context and thus captures the context than just meaning of \\nthe word unlike in GloVe and Word2Vec. Nltk is not a word embedding. \\n \\n \\n \\n \\nQ30 For a given token, its input representation is the sum of embedding from the \\ntoken, segment and position embedding \\na. ELMo \\nb. GPT \\nc. BERT \\nd. ULMFit \\n \\nAnswer :  c) BERT uses token, segment and position embedding. \\n \\n \\nQ31. Trains two independent LSTM language model left to right and right to left and \\nshallowly concatenates them \\na. GPT \\nb. BERT \\nc. ULMFit \\nd. ELMo \\n'),\n",
       " Document(metadata={'source': 'NLP.pdf', 'page': 8}, page_content='Steve Nouri                          https://www.linkedin.com/in/stevenouri/   \\nAnswer : d) ELMo tries to train two independent LSTM language models (left to right  and right to \\nleft) and concatenates the results to produce word embedding. \\n \\nQ32. Uses unidirectional language model for producing word embedding \\na. BERT \\nb. GPT \\nc. ELMo \\nd. Word2Vec \\n \\nAnswer : b) GPT is a unidirectional model and word embedding are produced by training on \\ninformation flow from left to right. ELMo is bidirectional but shall ow. Word2Vec provides simple \\nword embedding. \\n \\nQ33. In this architecture, the relationship between all words in a sentence is \\nmodelled irrespective of their position. Which architecture is this? \\na. OpenAI GPT \\nb. ELMo \\nc. BERT \\nd. ULMFit \\n \\nAnswer : c)BERT Transformer architecture models the relationship between each word and all \\nother words in the sentence to generate attention scores. These attention scores are later used \\nas weights for a weighted average of all words’ representations which is fed into a f ully-connected \\nnetwork to generate a new representation. \\n \\nQ34. List 10 use cases to be solved using NLP techniques? \\n● Sentiment Analysis \\n● Language Translation (English to German, Chinese to English, etc..)  \\n● Document Summarization \\n● Question Answering \\n● Sentence Completion \\n● Attribute extraction (Key information extraction from the documents) \\n● Chatbot interactions \\n● Topic classification \\n● Intent extraction \\n● Grammar or Sentence correction \\n● Image captioning \\n● Document Ranking \\n● Natural Language inference \\n \\nQ35. Transformer model pays attention to the most important word in Sentence \\na. True \\nb. False \\n'),\n",
       " Document(metadata={'source': 'NLP.pdf', 'page': 9}, page_content='Steve Nouri                          https://www.linkedin.com/in/stevenouri/  Answer : a) Attention mechanisms in the Transformer model are used to model the relatio nship \\nbetween all words and also provide weights to the most important word. \\n \\nQ36. Which NLP model gives the best accuracy amongst the following? \\na. BERT \\nb. XLNET \\nc. GPT-2 \\nd. ELMo \\n \\nAnswer : b) XLNET has given best accuracy amongst all the models. It has outperformed B ERT \\non 20 tasks and achieves state of art results on 18 tasks including sentiment an alysis, question \\nanswering, natural language inference, etc. \\n \\nQ37. Permutation Language models is a feature of \\na. BERT \\nb. EMMo \\nc. GPT \\nd. XLNET \\n \\nAnswer : d) XLNET provides permutation-based language modelling and is a key difference from \\nBERT. In permutation language modeling, tokens are predicted in a random manner and no t \\nsequential. The order of prediction is not necessarily left to right and can be right to left. The \\noriginal order of words is not changed but a prediction can be random.  \\nThe conceptual difference between BERT and XLNET can be seen from the following diagram. \\n \\nQ38. Transformer XL uses relative positional embedding \\na. True \\nb. False \\na) Instead of embedding having to represent the absolute position of a word, Transfo rmer XL uses \\nan embedding to encode the relative distance between the words. This embedding is use d to \\ncompute the attention score between any 2 words that could be separated by n words bef ore or \\nafter. \\n \\nQ39. What is Naive Bayes algorithm, When we can use this algorithm in NLP? \\nNaive Baye s algorithm is a collection of classifiers which works on the principles of the Baye s’ \\ntheorem. This series of NLP model forms a family of algorithms that can  be used for a wide range \\nof classification tasks including sentiment prediction, filtering of spam, classifying documents and \\nmore. \\nNaive Bayes algorithm converges faster and requires less training data. Compared  to other \\ndiscriminative models like logistic regression, Naive Bayes model it takes lesser t ime to train. This \\nalgorithm is perfect for use while working with multiple classes and text classificatio n where the \\ndata is dynamic and changes frequently. \\n \\nQ40. Explain Dependency Parsing in NLP? \\n'),\n",
       " Document(metadata={'source': 'NLP.pdf', 'page': 10}, page_content='Steve Nouri                          https://www.linkedin.com/in/stevenouri/  Dependency Parsing, also known as Syntactic parsing in NLP is a process of assigning syntactic \\nstructure to a sentence and identifying its dependency parses. This process is crucial to \\nunderstand the correlations between the “head” words in the syntactic structure.  \\nThe process of dependency parsing can be a little complex considering how any senten ce can \\nhave more than one dependency parses. Multiple parse trees are known as ambiguit ies. \\nDependency parsing needs to resolve these ambiguities in order to effectively assign a syntactic \\nstructure to a sentence. \\nDependency parsing can be used in the semantic analysis of a sentence apart from t he syntactic \\nstructuring. \\n \\nQ41. What is text Summarization? \\nText summarization is the process of shortening a long piece of text with its meani ng and effect \\nintact. Text summarization intends to create a summary of any given piece of text and outlines \\nthe main points of the document. This technique has improved in recent times and i s capable of \\nsummarizing volumes of text successfully. \\nText summarization has proved to a blessing since machines can summarise large volumes of \\ntext in no time which would otherwise be really time-consuming. There are two types of text \\nsummarization: \\n● Extraction-based summarization \\n● Abstraction-based summarization \\n \\n \\nQ42. What is NLTK? How is it different from Spacy? \\nNLTK or Natural Language Toolkit is a series of libraries and programs that are used f or symbolic \\nand statistical natural language processing. This toolkit contains some of the most po werful \\nlibraries that can work on different ML techniques to break down and understand hum an \\nlanguage. NLTK is used for Lemmatization, Punctuation, Character count, Tokenizati on, and \\nStemming. The difference between NLTK and Spacey are as follows: \\n● While NLTK has a collection of programs to choose from, Spacey contains only the best -\\nsuited algorithm for a problem in its toolkit \\n● NLTK supports a wider range of languages compared to Spacey (Spacey support s only 7 \\nlanguages) \\n● While Spacey has an object-oriented library, NLTK has a string processing library \\n● Spacey can support word vectors while NLTK cannot \\n \\nQ43. What is information extraction? \\nInformation extraction in the context of Natural Language Processing refers to t he technique of \\nextracting structured information automatically from unstructured sources t o ascribe meaning to \\nit. This can include extracting information regarding attributes of entities, relat ionship between \\ndifferent entities and more. The various models of information extraction includes: \\n● Tagger Module \\n● Relation Extraction Module \\n● Fact Extraction Module \\n● Entity Extraction Module \\n'),\n",
       " Document(metadata={'source': 'NLP.pdf', 'page': 11}, page_content='Steve Nouri                          https://www.linkedin.com/in/stevenouri/  ● Sentiment Analysis Module \\n● Network Graph Module \\n● Document Classification & Language Modeling Module \\n \\nQ44. What is Bag of Words? \\nBag of Words is a commonly used model that depends on word frequencies or occurrences to \\ntrain a classifier. This model creates an occurrence matrix for documents or senten ces \\nirrespective of its grammatical structure or word order.  \\n \\nQ45. What is Pragmatic Ambiguity in NLP? \\nPragmatic ambiguity refers to those words which have more than one meaning and their use in \\nany sentence can depend entirely on the context. Pragmatic ambiguity can resul t in multiple \\ninterpretations of the same sentence. More often than not, we come across sentences which  have \\nwords with multiple meanings, making the sentence open to interpretation. This mul tiple \\ninterpretation causes ambiguity and is known as Pragmatic ambiguity in NLP. \\n \\nQ46. What is a Masked Language Model? \\nMasked language models help learners to understand deep representations in downstr eam tasks \\nby taking an output from the corrupt input. This model is often used to p redict the words to be \\nused in a sentence. \\nQ48. What are the best NLP Tools? \\nSome of the best NLP tools from open sources are: \\n● SpaCy \\n● TextBlob \\n● Textacy \\n● Natural language Toolkit \\n● Retext \\n● NLP.js \\n● Stanford NLP \\n● CogcompNLP \\n \\nQ49. What is POS tagging? \\nParts of speech tagging better known as POS tagging refers to the process of i dentifying specific \\nwords in a document and group them as part of speech, based on its context. POS tagging is also \\nknown as grammatical tagging since it involves understanding grammatical structures and \\nidentifying the respective component. \\nPOS tagging is a complicated process since the same word can be different parts of speech \\ndepending on the context. The same generic process used for word mapping is qui te ineffective \\nfor POS tagging because of the same reason. \\n \\nQ50. What is NES? \\nName entity recognition is more commonly known as NER is the process of identifying speci fic \\nentities in a text document which are more informative and have a unique context.  These often \\ndenote places, people, organisations, and more. Even though it seems like these entit ies are \\n'),\n",
       " Document(metadata={'source': 'NLP.pdf', 'page': 12}, page_content='Steve Nouri                          https://www.linkedin.com/in/stevenouri/  proper nouns, the NER process is far from identifying just the nouns. In fact , NER involves entity \\nchunking or extraction wherein entities are segmented to categorise them under differen t \\npredefined classes. This step further helps in extracting information. \\n \\nQ51 Explain the Masked Language  Model? \\nMasked language modelling is the process in which the output is taken from t he corrupted input. \\nThis model helps the learners to master the deep representations in downstream  tasks. You can \\npredict a word from the other words of the sentence using this model. \\n \\nQ52 What is pragmatic analysis in NL P? \\nPragmatic Analysis: It deals with outside word knowledge, which means knowledge  that is \\nexternal to the documents and/or queries. Pragmatics analysis that focuses on what was \\ndescribed is reinterpreted by what it actually meant, deriving the various aspects  of language that \\nrequire real-world knowledge. \\n \\n \\n \\n \\nQ53 What is perplexity in NLP?  \\nThe word \"perplexed\" means \"puzzled\" or \"confused\", thus Perplexity in general means t he \\ninability to tackle something complicated and a problem that is not specified. Therefore, Pe rplexity \\nin NLP is a way to determine the extent of uncertainty in predicting some text. \\nIn NLP, perplexity is a way of evaluating language models. Perplexity can be high and low; Low \\nperplexity is ethical because the inability to deal with any complicated problem i s less while high \\nperplexity is terrible because the failure to deal with a complicated is high. \\n \\nQ54 What is ngram in NLP?  \\nN-gram in NLP is simply a sequence of n words, and we also conclude the sentences which \\nappeared more frequently, for example, let us consider the progression of these three wo rds: \\n● New York (2 gram) \\n● The Golden Compass (3 gram) \\n● She was there in the hotel (4 gram) \\nNow from the above sequence, we can easily conclude that sentence (a) appeared more \\nfrequently than the other two sentences, and the last sentence(c) is not seen that  often. Now if \\nwe assign probability in the occurrence of an n-gram, then it will be advan tageous. It would help \\nin making next-word predictions and in spelling error corrections. \\n \\nQ55 Explain differences between AI, Machine Learning and NLP \\n'),\n",
       " Document(metadata={'source': 'NLP.pdf', 'page': 13}, page_content='Steve Nouri                          https://www.linkedin.com/in/stevenouri/   \\n \\n \\n \\n \\n \\n \\nQ56 Why self-attention is awesome? \\n“In terms of computational complexity, self -attention layers are faster than recurrent layers when \\nthe sequence length n is smaller than the representation dimensionality d, which i s most often the \\ncase with sentence representations used by state- of-the-art models in machine translations, such \\nas word-piece and byte- pair representations.” —  from Attention is all you need \\n \\nQ57 What are stop words? \\nStop words are said to be useless data for a search engine. Words such as articles,  prepositions, \\netc. are considered as stop words. There are stop words such as was, were, is, am, the, a , an, \\nhow, why, and many more. In Natural Language Processing, we eliminate the stop words to \\nunderstand and analyze the meaning of a sentence. The removal of stop words is one of the most \\nimportant tasks for search engines. Engineers design the algorithms of search engines  in such a \\nway that they ignore the use of stop words. This helps show the relevant search result f or a query. \\n \\nQ58 What is Latent Semantic Indexing (LSI)? \\n'),\n",
       " Document(metadata={'source': 'NLP.pdf', 'page': 14}, page_content='Steve Nouri                          https://www.linkedin.com/in/stevenouri/  Latent semantic indexing is a mathematical technique used to improve the accur acy of the \\ninformation retrieval process. The design of LSI algorithms allows machines to de tect the hidden \\n(latent) correlation between semantics (words). To enhance information understanding, m achines \\ngenerate various concepts that associate with the words of a sentence. \\nThe technique used for information understanding is called singular value decomposition. I t is \\ngenerally used to handle static and unstructured data. The matrix obtained for singular val ue \\ndecomposition contains rows for words and columns for documents. This method best suits t o \\nidentify components and group them according to their types. \\nThe main principle behind LSI is that words carry a similar meaning when used i n a similar context. \\nComputational LSI models are slow in comparison to other models. However, they  are good at \\ncontextual awareness that helps improve the analysis and understanding of a text or a document. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nQ60 What are Regular Expressions? \\nA regular expression is used to match and tag words. It consists of a series of cha racters for \\nmatching strings. \\nSuppose, if A and B are regular expressions, then the following are true for them:  \\n● If { ɛ} is a regular language, then ɛ is a regular expression for it. \\n● If A and B are regular expressions, then A + B is also a regular expression withi n the \\nlanguage {A, B}. \\n● If A and B are regular expressions, then the concatenation of A and B (A.B) is a  regular \\nexpression. \\n● If A is a regular expression, then A* (A occurring multiple times) is also a regular \\nexpression. \\n \\nQ61 What are unigrams, bigrams, trigrams, and n-grams in NLP? \\nWhen we parse a sentence one word at a time, then it is called a unigram. The sentence parsed \\ntwo words at a time is a bigram. \\nWhen the sentence is parsed three words at a time, then it is a trigram. Si milarly, n-gram refers \\nto the parsing of n words at a time. \\nExample: To understand unigrams, bigrams, and trigrams, you can refer to the below diag ram: \\n \\nQ62 What are the steps involved in solving an NLP problem? \\nBelow are the steps involved in solving an NLP problem: \\n1. Gather the text from the available dataset or by web scraping \\n'),\n",
       " Document(metadata={'source': 'NLP.pdf', 'page': 15}, page_content='Steve Nouri                          https://www.linkedin.com/in/stevenouri/  2. Apply stemming and lemmatization for text cleaning \\n3. Apply feature engineering techniques \\n4. Embed using word2vec \\n5. Train the built model using neural networks or other Machine Learning techniques \\n6. Evaluate the model’s performance  \\n7. Make appropriate changes in the model \\n8. Deploy the model \\n \\nQ63. There have some various common elements of natural language processing. \\nThose elements are very important for understanding NLP properly, can you please \\nexplain the same in details with an example? \\nAnswer: \\nThere have a lot of components normally using by natural language processing (NLP). Some  of \\nthe major components are explained below: \\n● Extraction of Entity: It actually identifying and extracting some critical data from the \\navailable information which help to segmentation of provided sentence on identifying ea ch \\nentity. It can help in identifying one human that it’s fictional or real, same kind of real ity \\nidentification for any organization, events or any geographic location etc. \\n● The analysis in a syntactic way: it mainly helps for maintaining ordering properly of the  \\navailable words. \\nQ64 In the case of processing natural language, we normally mentioned one \\ncommon terminology NLP and binding every language with the same terminology \\nproperly. Please explain in details about this NLP terminology with an example? \\nAnswer: \\nThis is the basic NLP Interview Questions asked in an interview. There have some several factors \\navailable in case of explaining natural language processing. Some of the key factors are given  \\nbelow: \\n● Vectors and Weights: Google Word vectors, length of TF-IDF, varieties documents, w ord \\nvectors, TF-IDF. \\n● Structure of Text: Named Entities, tagging of part of speech, identifying the head of the \\nsentence. \\n● Analysis of sentiment: Know about the features of sentiment, entities available for the \\nsentiment, sentiment common dictionary. \\n● Classification of Text: Learning supervising, set off a train, set of validation  in Dev, Set of \\ndefine test, a feature of the individual text, LDA. \\n● Reading of Machine Language: Extraction of the possible entity, linking with an individual \\nentity, DBpedia, some libraries like Pikes or FRED. \\n \\nQ65 Explain briefly about word2vec \\nWord2Vec  embeds words in a lower-dimensional vector space using a shallow neural networ k. \\nThe result is a set of word-vectors where vectors close together in vector space have similar \\nmeanings based on context, and word-vectors distant to each other have differing mea nings. For \\nexample, apple and orange would be close together and apple and gravity would be relative ly far. \\n'),\n",
       " Document(metadata={'source': 'NLP.pdf', 'page': 16}, page_content='Steve Nouri                          https://www.linkedin.com/in/stevenouri/  There are two versions of this model based on skip-grams (SG) and continuous- bag-of-words \\n(CBOW). \\n \\nQ66 What are the metrics used to test an NLP model? \\nAccuracy, Precision, Recall and F1. Accuracy is the usual ratio of the prediction t o the desired \\noutput. But going just be accuracy is naive considering the complexities involved.  \\n \\nQ67 What are some ways we can preprocess text input? \\nHere are several preprocessing steps that are commonly used for NLP tasks: \\n● case normalization: we can convert all input to the same case (lowercase or uppercase) \\nas a way of reducing our text to a more canonical form \\n● punctuation/stop word/white space/special characters removal: if we don’t think these \\nwords or characters are relevant, we can remove them to reduce the feature space \\n● lemmatizing/stemming: we can also reduce words to their inflecti onal forms (i.e. walks → \\nwalk) to further trim our vocabulary \\n● generalizing irrelevant information: we can replace all numbers with a <NUMBER > token \\nor all names with a <NAME> token \\n \\n \\nQ68 How does the encoder-decoder structure work for language modelling? \\nThe encoder-decoder structure is a deep learning model architecture responsible for several s tate \\nof the art solutions, including Machine Translation. \\nThe input sequence is passed to the encoder where it is transformed to a fixed -dimensional vector \\nrepresentation using a neural network. The transformed input is then decoded using anothe r \\nneural network. Then, these outputs undergo another transformation and a softmax l ayer. The \\nfinal output is a vector of probabilities over the vocabularies. Meaningful informatio n is extracted \\nbased on these probabilities. \\n \\nQ69 What are attention mechanisms and why do we use them? \\nThis was a followup to the encoder-decoder question. Only the output from  the last time step is \\npassed to the decoder, resulting in a loss of information learned at previou s time steps. This \\ninformation loss is compounded for longer text sequences with more time steps. \\nAttention mechanisms are a function of the hidden weights at each time  step. When we use \\nattention in encoder-decoder networks, the fixed-dimensional vector passed to t he decoder \\nbecomes a function of all vectors outputted in the intermediary steps. \\nTwo commonly used attention mechanisms are additive attention and multiplicative attention. A s \\nthe names suggest, additive attention is a weighted sum while multiplicative attention i s a \\nweighted multiplier of the hidden weights. During the training process, the m odel also learns \\nweights for the attention mechanisms to recognize the relative importance of each time step. \\n \\nQ70 How would you implement an NLP system as a service, and what are some \\npitfalls you might face in production? \\nThis is less of a NLP question than a question for productionizing machine learning models. There \\nare however certain intricacies to NLP models. \\n'),\n",
       " Document(metadata={'source': 'NLP.pdf', 'page': 17}, page_content='Steve Nouri                          https://www.linkedin.com/in/stevenouri/  Without diving too much into the productionization aspect, an ideal Machine Learning service wi ll \\nhave: \\n● endpoint(s) that other business systems can use to make inference \\n● a feedback mechanism for validating model predictions \\n● a database to store predictions and ground truths from the feedback \\n● a workflow orchestrator which will (upon some signal) re-train and load the ne w model for \\nserving based on the records from the database + any prior training data \\n● some form of model version control to facilitate rollbacks in case of bad deployments \\n● post- production accuracy and error monitoring \\n \\n \\n \\n \\n \\n \\n \\n \\nQ71 How can we handle misspellings for text input? \\nBy using word embeddings trained over a large corpus (for instance, an extensive web scrape of \\nbillions of words), the model vocabulary would include common misspellings by design. The \\nmodel can then learn the relationship between misspelled and correctly spelled words to \\nrecognize their semantic similarity. \\nWe can also preprocess the input to prevent misspellings. Terms not found in the m odel \\nvocabulary can be mapped to the “closest” vocabulary term using:  \\n● edit distance between strings \\n● phonetic distance between word pronunciations \\n● keyword distance to catch common typos \\n \\nQ72 Which of the following models can perform tweet classification with regards \\nto context mentioned above? \\nA) Naive Bayes \\nB) SVM \\nC) None of the above \\nSolution: (C) \\nSince, you are given only the data of tweets and no other information, whi ch means there is no \\ntarget variable present. One cannot train a supervised learning model, both svm and naive bayes \\nare supervised learning techniques. \\n \\nQ73 You have created a document term matrix of the data, treating every tweet as \\none document. Which of the following is correct, in regards to document term \\nmatrix? \\n1. Removal of stopwords from the data will affect the dimensionality of data \\n2. Normalization of words in the data will reduce the dimensionality of data \\n'),\n",
       " Document(metadata={'source': 'NLP.pdf', 'page': 18}, page_content='Steve Nouri                          https://www.linkedin.com/in/stevenouri/  3. Converting all the words in lowercase will not affect the dimensionality of the  data \\nA) Only 1 \\nB) Only 2 \\nC) Only 3 \\nD) 1 and 2 \\nE) 2 and 3 \\nF) 1, 2 and 3 \\nSolution: (D) \\nChoices A and B are correct because stopword removal will decrease the number of feature s in \\nthe matrix, normalization of words will also reduce redundant features, and, convert ing all words \\nto lowercase will also decrease the dimensionality. \\n  \\n \\n \\n \\n \\nQ74 Which of the following features can be used for accuracy improvement of a \\nclassification model? \\nA) Frequency count of terms \\nB) Vector Notation of sentence \\nC) Part of Speech Tag \\nD) Dependency Grammar \\nE) All of these \\nSolution: (E) \\nAll of the techniques can be used for the purpose of engineering features in a model. \\n  \\nQ75 What percentage of the total statements are correct with regards to Topic \\nModeling? \\n1. It is a supervised learning technique \\n2. LDA (Linear Discriminant Analysis) can be used to perform topic modeling \\n3. Selection of number of topics in a model does not depend on the size of data \\n4. Number of topic terms are directly proportional to size of the data \\nA) 0 \\nB) 25 \\nC) 50 \\nD) 75 \\nE) 100 \\nSolution: (A) \\nLDA is unsupervised learning model, LDA is latent Dirichlet allocation, not Linear discriminant \\nanalysis. Selection of the number of topics is directly proportional to the size of the dat a, while \\nnumber of topic terms is not directly proportional to the size of the data. Hence no ne of the \\nstatements are correct. \\n  \\n'),\n",
       " Document(metadata={'source': 'NLP.pdf', 'page': 19}, page_content=\"Steve Nouri                          https://www.linkedin.com/in/stevenouri/  Q76 In Latent Dirichlet Allocation model for text classification purposes, what does \\nalpha and beta hyperparameter represent- \\nA) Alpha: number of topics within documents, beta: number of terms within topi cs False \\nB) Alpha: density of terms generated within topics, beta: density of top ics generated within terms \\nFalse \\nC) Alpha: number of topics within documents, beta: number of terms within topi cs False \\nD) Alpha: density of topics generated within documents, beta: density of term s generated within \\ntopics True \\nSolution: (D) \\nOption D is correct \\n \\n \\n \\n \\n \\nQ77 What is the problem with ReLu? \\n● Exploding gradient(Solved by gradient clipping) \\n● Dying ReLu — No learning if the activation is 0 (Solved by parametric rel u) \\n● Mean and variance of activations is not 0 and 1.(Partially solved by subtracting around 0.5 \\nfrom activation. Better explained in fastai videos) \\n \\nQ78 What is the difference between learning latent features using SVD and getting \\nembedding vectors using deep network? \\nSVD uses linear combination of inputs while a neural network uses nonlinear combination . \\n \\nQ79 What is the information in the hidden and cell state of LSTM? \\nHidden stores all the information till that time step and cell state stores  particular information that \\nmight be needed in the future time step. \\nNumber of parameters in an LSTM model with bias \\n4(𝑚h+h²+h) where 𝑚 is input vectors size and h is output vectors size a.k.a. hidden \\nThe point to see here is that mh dictates the model size as m>>h. Hence  it's important to have a \\nsmall vocab. \\nTime complexity of LSTM \\nseq_length*hidden² \\nTime complexity of transfomer \\nseq_length²*hidden \\nWhen hidden size is more than the seq_length(which is normally the case), transfomer is faster \\nthan LSTM. \\n \\nQ80 When is self-attention not faster than recurrent layers? \\nWhen the sequence length is greater than the representation dimensions. This is rare.  \\n \\nQ81 What is the benefit of learning rate warm- up? \\n\"),\n",
       " Document(metadata={'source': 'NLP.pdf', 'page': 20}, page_content='Steve Nouri                          https://www.linkedin.com/in/stevenouri/  Learning rate warm-up is a learning rate schedule where you have low (or lower) learning rate at \\nthe beginning of training to avoid divergence due to unreliable gradients at the beginning. As t he \\nmodel becomes more stable, the learning rate would increase to speed up convergence. \\n \\nQ82 What’s the difference between hard and soft parameter sharing in multi -task \\nlearning? \\nHard sharing is where we train for all the task at the same time and update our weights using all \\nthe losses whereas soft sharing is where we train for one task at a time. \\n \\nQ83 What’s the difference between BatchNorm and LayerNorm?  \\nBatchNorm computes the mean and variance at each layer for every minibatch whereas \\nLayerNorm computes the mean and variance for every sample for each layer independently.  \\nBatch normalisation allows you to set higher learning rates, increasing speed of  training as it \\nreduces the unstability of initial starting weights. \\nQ84 Difference between BatchNorm and LayerNorm? \\nBatchNorm — Compute the mean and var at each layer for every minibatch \\nLayerNorm — Compute the mean and var for every single sample for each layer independently \\n \\nQ85 Why does the transformer block have LayerNorm instead of BatchNorm? \\nLooking at the advantages of LayerNorm, it is robust to batch size and works better as i t works at \\nthe sample level and not batch level. \\n \\nQ86 What changes would you make to your deep learning code if you knew there  \\nare errors in your training data? \\nWe can do label smoothening where the smoothening value is based on % error. If any particular \\nclass has known error, we can also use class weights to modify the loss. \\n \\nQ87 What are the tricks used in ULMFiT? (Not a great questions but checks the \\nawareness) \\n● LM tuning with task text \\n● Weight dropout \\n● Discriminative learning rates for layers \\n● Gradual unfreezing of layers \\n● Slanted triangular learning rate schedule \\nThis can be followed up with a question on explaining how they help. \\n \\nQ88 Tell me a language model whi ch doesn’t use dropout  \\nALBERT v2 — This throws a light on the fact that a lot of assumptions we take for granted are  \\nnot necessarily true. The regularisation effect of parameter sharing in ALBERT is so strong that \\ndropouts are not needed. (ALBERT v1 had dropouts.) \\n \\nQ89 What are the differences between GPT and GPT-2? (From Lilian Weng) \\n'),\n",
       " Document(metadata={'source': 'NLP.pdf', 'page': 21}, page_content='Steve Nouri                          https://www.linkedin.com/in/stevenouri/  ● Layer normalization  was moved to the input of each sub-block, similar to a residual unit of \\ntype “building block”  (differently from the original type  “bottleneck” , it has batch \\nnormalization applied before weight layers). \\n● An additional layer normalization was added after the final self-attention block. \\n● A modified initialization was constructed as a function of the model depth. \\n● The weights of residual layers were initially sca led by a factor of 1/√n where n is the \\nnumber of residual layers. \\n● Use larger vocabulary size and context size. \\n \\nQ90 What are the differences between GPT and BERT? \\n \\n● GPT is not bidirectional and has no concept of masking \\n● BERT adds next sentence prediction task in training and so it also has a seg ment \\nembedding \\nQ91 What are the differences between BERT and ALBERT v2? \\n● Embedding matrix factorisation(helps in reducing no. of parameters) \\n● No dropout \\n● Parameter sharing(helps in reducing no. of parameters and regularisation) \\nQ92 How does parameter sharing in ALBERT affect the training and inference time? \\nNo effect. Parameter sharing just decreases the number of parameters. \\nQ93 How would you reduce the inference time of a trained NN model? \\n● Serve on GPU/TPU/FPGA \\n● 16 bit quantisation and served on GPU with fp16 support \\n● Pruning to reduce parameters \\n● Knowledge distillation (To a smaller transformer model or simple neural network) \\n● Hierarchical softmax/Adaptive softmax \\n● You can also cache results as explained here. \\nQ94 Would you use BPE with classical models? \\nOf course! BPE is a smart tokeniser and it can help us get a smaller vocabu lary which can help \\nus find a model with less parameters. \\nQ95 How would you make an arxiv papers search engine? (I was asked — How \\nwould you make a plagiarism detector?) \\n \\nGet top k results with TF-IDF similarity and then rank results with \\n● semantic encoding + cosine similarity \\n● a model trained for ranking \\n \\n'),\n",
       " Document(metadata={'source': 'NLP.pdf', 'page': 22}, page_content='Steve Nouri                          https://www.linkedin.com/in/stevenouri/  Q96 Get top k results with TF-IDF similarity and then rank results with \\n● semantic encoding + cosine similarity \\n● a model trained for ranking \\nQ97 How would you make a sentiment classifier? \\nThis is a trick question. The interviewee can say all things such as using transfer l earning and \\nlatest models but they need to talk about having a neutral class too otherwise you can have really \\ngood accuracy/f1 and still, the model will classify everything into positive or nega tive. \\nThe truth is that a lot of news is neutral and so the training need s to have this class. The \\ninterviewee should also talk about how he will create a dataset and his training st rategies like the \\nselection of language model, language model fine-tuning and using various datasets for multi-\\ntask learning. \\nQ98 What is the difference between regular expression and regular grammar? \\nA regular expression is the representation of natural language in the form of mathe matical \\nexpressions containing a character sequence. On the other hand, regular grammar is th e \\ngenerator of natural language, defining a set of defined rules and syntax which the strings in the \\nnatural l anguage must follow.  \\n \\nQ99 Why should we use Batch Normalization? \\nOnce the interviewer has asked you about the fundamentals of deep learning architect ures, they \\nwould move on to the key topic of improving your deep learning model’s perform ance.  \\nBatch Normalization is one of the techniques used for reducing the training time of our deep \\nlearning algorithm. Just like normalizing our input helps improve our logist ic regression model, we \\ncan normalize the activations of the hidden layers in our deep learning model as well: \\n \\nQ100 How is backpropagation different in RNN compared to ANN? \\nIn Recurrent Neural Networks, we have an additional loop at each node: \\nThis loop essentially includes a time component into the network as well. This helps in capturing \\nsequential information from the data, which could not be possible in a generic art ificial neural \\nnetwork. \\nThis is why the backpropagation in RNN is called Backpropagation through Time, as in  \\nbackpropagation at each time step. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader('NLP.pdf')\n",
    "pdf_document = loader.load()\n",
    "pdf_document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>convert the document into chunks</h1?>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'NLP.pdf', 'page': 0}, page_content='Steve Nouri                          https://www.linkedin.com/in/stevenouri/  Top 100 NLP Questions \\n     Steve Nouri  \\n \\n \\nQ1. Which of the following techniques can be used for keyword normalization in \\nNLP, the process of converting a keyword into its base form? \\na. Lemmatization \\nb. Soundex \\nc. Cosine Similarity \\nd. N-grams \\n \\nAnswer : a) Lemmatization helps to get to the base form of a word, e.g. are playing -> play, ea ting \\n-> eat, etc.Other options are meant for different purposes. \\n \\nQ2. Which of the following techniques can be used to compute the distance \\nbetween two word vectors in NLP? \\na. Lemmatization \\nb. Euclidean distance \\nc. Cosine Similarity \\nd. N-grams \\n \\nAnswer : b) and c) \\nDistance between two word vectors can be computed using Cosine similarity and Euclidean \\nDistance.  Cosine Similarity establishes a cosine angle between the vector of two words . A cosi ne \\nangle close to each other between two word vectors indicates the words are simil ar and vice a \\nversa.'),\n",
       " Document(metadata={'source': 'NLP.pdf', 'page': 0}, page_content='angle close to each other between two word vectors indicates the words are simil ar and vice a \\nversa. \\nE.g. cosine angle between two words “Football” and “Cricket” will be closer to 1 as co mpared to \\nangle between the words “Football” and “New Delhi”  \\n \\nQ3. What are the possible features of a text corpus in NLP? \\na. Count of the word in a document \\nb. Vector notation of the word \\nc. Part of Speech Tag \\nd. Basic Dependency Grammar \\ne. All of the above \\n \\nAnswer : e)All of the above can be used as features of the text corpus. \\n \\n \\nQ4. You created a document term matrix on the input data of 20K documents for a \\nMachine learning model. Which of the following can be used to reduce the \\ndimensions of data?'),\n",
       " Document(metadata={'source': 'NLP.pdf', 'page': 1}, page_content='Steve Nouri                          https://www.linkedin.com/in/stevenouri/  1. Keyword Normalization \\n2. Latent Semantic Indexing \\n3. Latent Dirichlet Allocati on \\n \\na. only 1 \\nb. 2, 3 \\nc. 1, 3 \\nd. 1, 2, 3 \\n  \\nAnswer : d) \\n \\nQ5. Which of the text parsing techniques can be used for noun phrase detection, \\nverb phrase detection, subject detection, and object detection in NLP. \\na. Part of speech tagging \\nb. Skip Gram and N-Gram extraction \\nc. Continuous Bag of Words \\nd. Dependency Parsing and Constituency Parsing \\n \\nAnswer : d) \\n \\nQ6. Dissimilarity between words expressed using cosine similarity will have values \\nsignificantly higher than 0.5 \\na. True \\nb. False \\n \\nAnswer : a) \\n \\nQ7. Which one of the following are keyword Normalization techniques in NLP \\na.  Stemming \\nb.  Part of Speech \\nc. Named entity recognition \\nd. Lemmatization \\n \\nAnswer : a) and d) \\nPart of Speech (POS) and Named Entity Recognition(NER) are not keyword Norm alization'),\n",
       " Document(metadata={'source': 'NLP.pdf', 'page': 1}, page_content='a.  Stemming \\nb.  Part of Speech \\nc. Named entity recognition \\nd. Lemmatization \\n \\nAnswer : a) and d) \\nPart of Speech (POS) and Named Entity Recognition(NER) are not keyword Norm alization \\ntechniques. Named Entity help you extract Organization, Time, Date, City, etc..t ype of entities \\nfrom the given sentence, whereas Part of Speech helps you extract Noun, Verb, Pronoun, \\nadjective, etc..from the given sentence tokens. \\n \\nQ8. Which of the below are NLP use cases? \\na. Detecting objects from an image \\nb. Facial Recognition \\nc. Speech Biometric \\nd. Text Summarization')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_spliter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "pdf_chunks = text_spliter.split_documents(pdf_document)\n",
    "pdf_chunks[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Convert the documents into embeddings/indexing and store into vector db</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "db = Chroma.from_documents(pdf_chunks,OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Query on the vector db</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'increases the weight for words that are not used very much in a collection of \\ndocuments \\na. Term Frequency (TF) \\nb. Inverse Document Frequency (IDF) \\nc. Word2Vec \\nd. Latent Dirichlet Allocation (LDA) \\n \\nAnswer : b) \\n \\n \\n \\n \\n \\nQ11. In NLP, The process of removing words like “and”, “is”, “a”, “an”, “the” from \\na sentence is called as \\na. Stemming \\nb. Lemmatization \\nc. Stop word \\nd. All of the above \\n \\nAnswer : c) In Lemmatization, all the stop words such as a, an, the, etc.. are removed . One can \\nalso define custom stop words for removal.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = '''In NLP, which algorithm decreases the weight for commonly used words and \n",
    "increases the weight for words that are not used very much in a collection of \n",
    "documents ?'''\n",
    "result = db.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "db1 = FAISS.from_documents(pdf_chunks, OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
